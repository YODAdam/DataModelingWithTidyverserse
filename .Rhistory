data_token %>%
count(word, sort = TRUE) %>%
filter(n > 10000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
data_token %>%
count(word, sort = TRUE) %>%
filter(n > 50000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
word_by_compl <- data_token %>%
group_by(id_number, word) %>%
summarise(
w_in_compl = n()
) %>%
ungroup() %>%
left_join (
data_token %>%
group_by(id_number) %>%
summarise(
all_in_complaint = n()
)
)
data_token <- data_token %>%
filter(is_english)
data_token %>%
count(word, sort = TRUE) %>%
filter(n > 100000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
lemmatize_strings("payment")
lemmatize_strings("better")
lemmatize_strings("pay")
lemmatize_strings("runing")
lemmatize_strings("ran")
library(spacyr)
install.packages("spacyr")
library(spacyr)
spacy_initialize(model = "en_core_web_sm")
pacy_install()
spacy_install()
library(spacyr)
spacy_initialize(model = "en_core_web_sm")
txt <- c("running", "chatgpt", "neural", "banana", "bananaz")
parsed <- spacy_parse(txt, lemma = TRUE)
parsed[, c("token", "lemma")]
data_token %>%
count(word, sort = TRUE) %>%
filter(n > 100000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
data("stop_words")
stop_words
data_token <- data_token %>%
anti_join(stop_words)
data_token %>%
count(word, sort = TRUE) %>%
filter(n > 50000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
word_by_compl <- data_token %>%
group_by(id_number, word) %>%
summarise(
w_in_compl = n()
) %>%
ungroup() %>%
left_join (
data_token %>%
group_by(id_number) %>%
summarise(
all_in_complaint = n()
)
)
tf_idf_data <- data_token %>%
count(word, id_number, sort=TRUE) %>%
bind_tf_idf(word, id_number, n) %>%
arrange(desc(tf_idf))
tf_idf_data %>% write_rds(file = "data/tf_idf_data.rds")
tf_idf_data <- read_rds(file = "data/tf_idf_data.rds")
top_words <- tf_idf_data %>%
filter(str_length(word) <= 15) %>%
pull(var = word) %>% unique() %>%
str_subset("^[^\\d]" ) %>%
.[1:1000]
top_words <- tf_idf_data %>%
filter(str_length(word) <= 15) %>%
pull(var = word) %>% unique() %>%
str_subset("^[^\\d]" ) %>%
.[1:100]
tf_idf_data_long <- tf_idf_data %>%
filter(word %in% top_words) %>%
select(word, id_number, tf_idf) %>%
pivot_wider(id_cols = id_number, names_from = word, values_from = tf_idf, values_fill = 0.0)
tf_idf_data_long %>% write_rds(file = "data/tf_idf_data_long.rds")
View(tf_idf_data_long)
View(train_data)
#train_data$id_number <- 1:nrow(train_data)
final_train_data <- train_data %>%
select(id_number, Product) %>%
left_join(tf_idf_data_long) %>%
select(!id_number)
View(final_train_data)
data_token %>%
count(word, sort = TRUE) %>%
filter(n > 50000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
chosen_features <- data_token %>%
count(word, sort = TRUE)
chosen_features
chosen_features <- data_token %>%
count(word, sort = TRUE) %>% pull(word) %>% .[1:100]
chosen_features
tf_idf_data_long <- tf_idf_data %>%
filter(word %in% chosen_features) %>%
select(word, id_number, tf_idf) %>%
pivot_wider(id_cols = id_number, names_from = word, values_from = tf_idf, values_fill = 0.0)
#train_data$id_number <- 1:nrow(train_data)
final_train_data <- train_data %>%
select(id_number, Product) %>%
left_join(tf_idf_data_long) %>%
select(!id_number)
View(final_train_data)
library(tidymodels)
library(ranger)
# 1. Split the data (80% train / 20% test)
set.seed(123)
final_train_data <- final_train_data %>% mutate(across(!Product, \(x) {x[is.na(x)] <- 0}))
data_split <- initial_split(final_train_data, prop = 0.8, strata = Product)
View(final_train_data)
#train_data$id_number <- 1:nrow(train_data)
final_train_data <- train_data %>%
select(id_number, Product) %>%
left_join(tf_idf_data_long) %>%
select(!id_number)
library(tidymodels)
View(final_train_data)
library(ranger)
# 1. Split the data (80% train / 20% test)
set.seed(123)
final_train_data <- final_train_data %>% mutate(across(!Product, \(x) {x[is.na(x)] <- 0.0}))
View(final_train_data)
#train_data$id_number <- 1:nrow(train_data)
final_train_data <- train_data %>%
select(id_number, Product) %>%
left_join(tf_idf_data_long) %>%
select(!id_number)
View(final_train_data)
final_train_data %>% map_int(.f = \(x) {sum(is.na(x))})
tf_idf_data %>% filter(is.na(tf_idf))
tf_idf_data_long <- tf_idf_data %>%
filter(word %in% chosen_features) %>%
select(word, id_number, tf_idf) %>%
filter(is.na(tf_idf))
tf_idf_data %>%
filter(word %in% chosen_features) %>%
select(word, id_number, tf_idf) %>%
filter(is.na(tf_idf))
tf_idf_data_long <- tf_idf_data %>%
filter(word %in% chosen_features) %>%
select(word, id_number, tf_idf) %>%
pivot_wider(id_cols = id_number, names_from = word, values_from = tf_idf, values_fill = 0.0)
View(final_train_data)
final_train_data %>% map_chr(.is.character )
final_train_data %>% map_chr( is.character )
final_train_data %>% map_chr( is.character ) %>% sum()
final_train_data %>% map_logica( is.character ) %>% sum()
final_train_data %>% map_logical( is.character ) %>% sum()
final_train_data %>% map_lgl( is.character ) %>% sum()
# 1. Split the data (80% train / 20% test)
set.seed(123)
#final_train_data <- final_train_data %>% mutate(across(!Product, \(x) {x[is.na(x)] <- 0.0}))
data_split <- initial_split(final_train_data, prop = 0.8, strata = Product)
train_data <- training(data_split)
test_data  <- testing(data_split)
# 2. Recipe (prétraitement)
rf_recipe <- recipe(Product ~ ., data = train_data) %>%
step_impute_mean()
# 3. Spécification du modèle Random Forest
rf_spec <- rand_forest(
mtry = 5,       # à ajuster selon le nombre de variables explicatives
trees = 500,
min_n = 5
) %>%
set_engine("ranger", importance = "impurity") %>%  # importance des variables
set_mode("classification")
# 4. Workflow
rf_workflow <- workflow() %>%
add_recipe(rf_recipe) %>%
add_model(rf_spec)
# 5. Entraînement du modèle
rf_fit <- rf_workflow %>%
fit(data = train_data)
final_train_data_2 <- final_train_data %>% mutate(across(!Product, \(x) {x[is.na(x)] <- 0.0}))
View(final_train_data_2)
final_train_data_2 <- final_train_data %>% mutate(across(!Product, \(x) {x[is.na(x)] = .000}))
View(final_train_data_2)
final_train_data_2 <- final_train_data %>% mutate(across(!Product, \(x) {x * 100}))
View(final_train_data_2)
final_train_data_2 <- final_train_data %>% mutate(across(!Product, \(x) {x[is.na(x)] = 0.0}))
View(final_train_data_2)
final_train_data_2 <- final_train_data %>% mutate(across(!Product, ~replace_na(., 0.0)))
View(final_train_data_2)
#final_train_data_2 <- final_train_data %>% mutate(across(!Product, \(x) {x * 100}))
final_train_data <- final_train_data %>% mutate(across(!Product, ~replace_na(., 0.0)))
View(final_train_data_2)
final_train_data %>% map_lgl(.f = \(x) {sum(is.na(x))})
final_train_data %>% map_lgl(.f = \(x) {sum(is.na(x))}) %>% sum()
data_split <- initial_split(final_train_data, prop = 0.8, strata = Product)
train_data <- training(data_split)
test_data  <- testing(data_split)
# 2. Recipe (prétraitement)
rf_recipe <- recipe(Product ~ ., data = train_data) %>%
step_impute_mean()
# 3. Spécification du modèle Random Forest
rf_spec <- rand_forest(
mtry = 5,       # à ajuster selon le nombre de variables explicatives
trees = 500,
min_n = 5
) %>%
set_engine("ranger", importance = "impurity") %>%  # importance des variables
set_mode("classification")
# 4. Workflow
rf_workflow <- workflow() %>%
add_recipe(rf_recipe) %>%
add_model(rf_spec)
# 5. Entraînement du modèle
rf_fit <- rf_workflow %>%
fit(data = train_data)
# 6. Prédictions sur les données test
rf_preds <- predict(rf_fit, test_data) %>%
bind_cols(test_data)
# 7. Évaluation
metrics <- rf_preds %>%
mutate(
Product = factor(Product),
.pred_class = factor(.pred_class)
) %>%
metrics(truth = Product, estimate = .pred_class)
View(metrics)
conf_mat <- rf_preds %>%
conf_mat(truth = Product, estimate = .pred_class)
conf_mat <- rf_preds %>%
mutate(
Product = factor(Product),
.pred_class = factor(.pred_class)
) %>%
conf_mat(truth = Product, estimate = .pred_class)
View(conf_mat)
conf_mat[["table"]]
print(metrics)
print(conf_mat)
write_rds(x = rf_fit, file = "data/random_forest_fited.rds")
final_train_data %>% write_rds(file = "data/final_training_data.rds")
chosen_features <- data_token %>%
count(word, sort = TRUE) %>% pull(word) %>% .[1:20]
chosen_features
tf_idf_data_long <- tf_idf_data %>%
filter(word %in% chosen_features) %>%
select(word, id_number, tf_idf) %>%
pivot_wider(id_cols = id_number, names_from = word, values_from = tf_idf, values_fill = 0.0)
tf_idf_data_long <- tf_idf_data %>%
filter(word %in% chosen_features) %>%
select(word, id_number, tf_idf) %>%
pivot_wider(id_cols = id_number, names_from = word, values_from = tf_idf, values_fill = 0.0)
#train_data$id_number <- 1:nrow(train_data)
final_train_data <- train_data %>%
select(id_number, Product) %>%
left_join(tf_idf_data_long) %>%
select(!id_number)
train_data <- read_csv(file = "data/data_complaints_train.csv")
train_data %>%
glimpse()
testing_data <- read_csv(file = "data/data_complaints_test.csv")
testing_data %>% glimpse()
train_data %>%
slice(1) %>%
pull(var = `Consumer complaint narrative`)
train_data <- train_data %>%
mutate(
id_number = row_number()
)
tf_idf_data_long <- tf_idf_data %>%
filter(word %in% chosen_features) %>%
select(word, id_number, tf_idf) %>%
pivot_wider(id_cols = id_number, names_from = word, values_from = tf_idf, values_fill = 0.0)
#train_data$id_number <- 1:nrow(train_data)
final_train_data <- train_data %>%
select(id_number, Product) %>%
left_join(tf_idf_data_long) %>%
select(!id_number)
tf_idf_data_long <- tf_idf_data %>%
filter(word %in% chosen_features) %>%
select(word, id_number, tf_idf) %>%
pivot_wider(id_cols = id_number, names_from = word, values_from = tf_idf, values_fill = 0.0)
#train_data$id_number <- 1:nrow(train_data)
final_train_data <- train_data %>%
select(id_number, Product) %>%
left_join(tf_idf_data_long) %>%
select(!id_number)
#final_train_data_2 <- final_train_data %>% mutate(across(!Product, \(x) {x * 100}))
final_train_data <- final_train_data %>% mutate(across(!Product, ~replace_na(., 0.0)))
View(final_train_data)
final_train_data %>% write_rds(file = "data/final_training_data.rds")
data_split <- initial_split(final_train_data, prop = 0.8, strata = Product)
training_data <- training(data_split)
testing_data  <- testing(data_split)
# 2. Recipe (prétraitement)
rf_recipe <- recipe(Product ~ ., data = training_data) %>%
step_impute_mean()
# 3. Spécification du modèle Random Forest
rf_spec <- rand_forest(
mtry = 5,       # à ajuster selon le nombre de variables explicatives
trees = 500,
min_n = 5
) %>%
set_engine("ranger", importance = "impurity") %>%  # importance des variables
set_mode("classification")
# 4. Workflow
rf_workflow <- workflow() %>%
add_recipe(rf_recipe) %>%
add_model(rf_spec)
# 5. Entraînement du modèle
rf_fit <- rf_workflow %>%
fit(data = training_data)
# 6. Prédictions sur les données test
rf_preds <- predict(rf_fit, testing_data) %>%
bind_cols(testing_data)
# 7. Évaluation
metrics <- rf_preds %>%
mutate(
Product = factor(Product),
.pred_class = factor(.pred_class)
) %>%
metrics(truth = Product, estimate = .pred_class)
conf_mat <- rf_preds %>%
mutate(
Product = factor(Product),
.pred_class = factor(.pred_class)
) %>%
conf_mat(truth = Product, estimate = .pred_class)
print(metrics)
print(conf_mat)
plot(conf_mat)
data_token <- data_token %>%
filter(is_english)
```{r}
library(tidyverse)
library(tidytext)
library(tm)
library(dtplyr)
library(textstem)
library(tidymodels)
library(ranger)
library(tidyverse)
library(tidytext)
library(tm)
library(dtplyr)
library(textstem)
library(tidymodels)
library(ranger)
train_data <- read_csv(file = "data/data_complaints_train.csv")
getwd()
train_data <- read_csv(file = "data/data_complaints_train.csv")
train_data <- read_csv(file = "data/data_complaints_train.csv")
train_data <- read_csv(file = "data/data_complaints_train.csv")
getwd()
getwd()
setwd("C:/Users/hp/Documents/ETUDES/Modeling_data_tidyverse")
```{r, echo=FALSE}
library(tidyverse)
library(tidytext)
library(tm)
library(dtplyr)
library(textstem)
library(tidymodels)
library(ranger)
setwd("C:/Users/hp/Documents/ETUDES/Modeling_data_tidyverse")
library(tidyverse)
library(tidytext)
library(tm)
library(dtplyr)
library(textstem)
library(tidymodels)
library(ranger)
setwd("C:/Users/hp/Documents/ETUDES/Modeling_data_tidyverse")
train_data <- read_csv(file = "data/data_complaints_train.csv")
train_data <- read_csv(file = "data/data_complaints_train.csv")
train_data <- read_csv(file = "data/data_complaints_train.csv")
setwd("C:/Users/hp/Documents/ETUDES/Modeling_data_tidyverse")
getwd()
train_data <- read_csv(file = "data/data_complaints_train.csv")
getwd()
library(tidyverse)
library(tidytext)
library(tm)
library(dtplyr)
library(textstem)
library(tidymodels)
library(ranger)
setwd("C:/Users/hp/Documents/ETUDES/Modeling_data_tidyverse")
train_data <- read_csv(file = "data/data_complaints_train.csv")
testing_data <- read_csv(file = "data/data_complaints_test.csv")
# Examine the structure of the training data
train_data %>% glimpse()
train_data %>%
slice(1) %>%
pull(`Consumer complaint narrative`)
train_data <- train_data %>%
mutate(id_number = row_number())
# Tokenize the complaints into individual words
data_token <- train_data %>%
select(Product, id_number, `Consumer complaint narrative`) %>%
unnest_tokens(output = word, input = `Consumer complaint narrative`) %>%
select(Product, id_number, word) %>%
filter(
!str_detect(word, "xx"),        # Remove words with successive x's
!str_detect(word, "[0-9]"),     # Remove words containing numbers
nchar(word) <= 20               # Keep words with max length of 20
) %>%
mutate(
word = lemmatize_words(word),   # Reduce words to their root form
is_english = hunspell_check(word, dict = hunspell::dictionary("en_US"))
)
data_token <- data_token %>%
filter(is_english)
data_token %>%
count(word, sort = TRUE) %>%
filter(n > 100000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL, title = "Most Frequent Words in Complaint Narratives")
data("stop_words")
data_token <- data_token %>%
anti_join(stop_words)
# Re-examine word frequencies after removing stop words
data_token %>%
count(word, sort = TRUE) %>%
filter(n > 50000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL, title = "Most Frequent Words After Stop Word Removal")
chosen_features <- data_token %>%
count(word, sort = TRUE) %>% pull(word) %>% .[1:20]
tf_idf_data <- data_token %>%
count(word, id_number, sort=TRUE) %>%
bind_tf_idf(word, id_number, n) %>%
arrange(desc(tf_idf))
# Create a wide format dataset with TF-IDF values
tf_idf_data_long <- tf_idf_data %>%
filter(word %in% chosen_features) %>%
select(word, id_number, tf_idf) %>%
pivot_wider(id_cols = id_number, names_from = word, values_from = tf_idf, values_fill = 0.0)
final_train_data <- train_data %>%
select(id_number, Product) %>%
left_join(tf_idf_data_long) %>%
select(!id_number) %>%
mutate(across(!Product, ~replace_na(., 0.0)))
set.seed(123)
data_split <- initial_split(final_train_data, prop = 0.8, strata = Product)
training_data <- training(data_split)
testing_data  <- testing(data_split)
# Define the recipe for preprocessing
rf_recipe <- recipe(Product ~ ., data = training_data) %>%
step_impute_mean()
# Specify the Random Forest model
rf_spec <- rand_forest(
mtry = 5,       # Number of variables randomly sampled as candidates at each split
trees = 500,    # Number of trees in the forest
min_n = 5       # Minimum number of data points in a node
) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
# Create workflow and train the model
rf_workflow <- workflow() %>%
add_recipe(rf_recipe) %>%
add_model(rf_spec)
rf_fit <- rf_workflow %>%
fit(data = training_data)
# Generate predictions
rf_preds <- predict(rf_fit, testing_data) %>%
bind_cols(testing_data)
# Calculate performance metrics
metrics <- rf_preds %>%
mutate(
Product = factor(Product),
.pred_class = factor(.pred_class)
) %>%
metrics(truth = Product, estimate = .pred_class)
# Create confusion matrix
conf_mat <- rf_preds %>%
mutate(
Product = factor(Product),
.pred_class = factor(.pred_class)
) %>%
conf_mat(truth = Product, estimate = .pred_class)
print(metrics)
print(conf_mat)
